{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =-=-=-=-=-=-=-=-=-=-=\n",
    "# IMPORTS\n",
    "# =-=-=-=-=-=-=-=-=-=-= \n",
    "\n",
    "import pandas, spacy, textacy\n",
    "from spacy import displacy\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from spacy.lang.en import English\n",
    "from pathlib import Path # This is to export diSplacy SVGs to files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =-=-=-=-=-=-=-=-=-=-=\n",
    "# FUNCTIONS\n",
    "# =-=-=-=-=-=-=-=-=-=-= \n",
    "\n",
    "def string_test(s):\n",
    "    if s is None:\n",
    "        return ''\n",
    "    else:\n",
    "        return str(s)\n",
    "\n",
    "# SVO TRIPLES\n",
    "# =-=-=-=-=-=\n",
    "\n",
    "SUBJECTS = [\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"]\n",
    "OBJECTS = [\"dobj\", \"dative\", \"attr\", \"oprd\"]\n",
    "ADJECTIVES = [\"acomp\", \"advcl\", \"advmod\", \"amod\", \"appos\", \"nn\", \"nmod\", \"ccomp\", \"complm\",\n",
    "              \"hmod\", \"infmod\", \"xcomp\", \"rcmod\", \"poss\",\" possessive\"]\n",
    "COMPOUNDS = [\"compound\"]\n",
    "PREPOSITIONS = [\"prep\"]\n",
    "\n",
    "def getSubsFromConjunctions(subs):\n",
    "    moreSubs = []\n",
    "    for sub in subs:\n",
    "        # rights is a generator\n",
    "        rights = list(sub.rights)\n",
    "        rightDeps = {tok.lower_ for tok in rights}\n",
    "        if \"and\" in rightDeps:\n",
    "            moreSubs.extend([tok for tok in rights if tok.dep_ in SUBJECTS or tok.pos_ == \"NOUN\"])\n",
    "            if len(moreSubs) > 0:\n",
    "                moreSubs.extend(getSubsFromConjunctions(moreSubs))\n",
    "    return moreSubs\n",
    "\n",
    "def getObjsFromConjunctions(objs):\n",
    "    moreObjs = []\n",
    "    for obj in objs:\n",
    "        # rights is a generator\n",
    "        rights = list(obj.rights)\n",
    "        rightDeps = {tok.lower_ for tok in rights}\n",
    "        if \"and\" in rightDeps:\n",
    "            moreObjs.extend([tok for tok in rights if tok.dep_ in OBJECTS or tok.pos_ == \"NOUN\"])\n",
    "            if len(moreObjs) > 0:\n",
    "                moreObjs.extend(getObjsFromConjunctions(moreObjs))\n",
    "    return moreObjs\n",
    "\n",
    "def getVerbsFromConjunctions(verbs):\n",
    "    moreVerbs = []\n",
    "    for verb in verbs:\n",
    "        rightDeps = {tok.lower_ for tok in verb.rights}\n",
    "        if \"and\" in rightDeps:\n",
    "            moreVerbs.extend([tok for tok in verb.rights if tok.pos_ == \"VERB\"])\n",
    "            if len(moreVerbs) > 0:\n",
    "                moreVerbs.extend(getVerbsFromConjunctions(moreVerbs))\n",
    "    return moreVerbs\n",
    "\n",
    "def findSubs(tok):\n",
    "    head = tok.head\n",
    "    while head.pos_ != \"VERB\" and head.pos_ != \"NOUN\" and head.head != head:\n",
    "        head = head.head\n",
    "    if head.pos_ == \"VERB\":\n",
    "        subs = [tok for tok in head.lefts if tok.dep_ == \"SUB\"]\n",
    "        if len(subs) > 0:\n",
    "            verbNegated = isNegated(head)\n",
    "            subs.extend(getSubsFromConjunctions(subs))\n",
    "            return subs, verbNegated\n",
    "        elif head.head != head:\n",
    "            return findSubs(head)\n",
    "    elif head.pos_ == \"NOUN\":\n",
    "        return [head], isNegated(tok)\n",
    "    return [], False\n",
    "\n",
    "def isNegated(tok):\n",
    "    negations = {\"no\", \"not\", \"n't\", \"never\", \"none\"}\n",
    "    for dep in list(tok.lefts) + list(tok.rights):\n",
    "        if dep.lower_ in negations:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def findSVs(tokens):\n",
    "    svs = []\n",
    "    verbs = [tok for tok in tokens if tok.pos_ == \"VERB\"]\n",
    "    for v in verbs:\n",
    "        subs, verbNegated = getAllSubs(v)\n",
    "        if len(subs) > 0:\n",
    "            for sub in subs:\n",
    "                svs.append((sub.orth_, \"!\" + v.orth_ if verbNegated else v.orth_))\n",
    "    return svs\n",
    "\n",
    "def getObjsFromPrepositions(deps):\n",
    "    objs = []\n",
    "    for dep in deps:\n",
    "        if dep.pos_ == \"ADP\" and dep.dep_ == \"prep\":\n",
    "            objs.extend([tok for tok in dep.rights if tok.dep_  in OBJECTS or (tok.pos_ == \"PRON\" and tok.lower_ == \"me\")])\n",
    "    return objs\n",
    "\n",
    "def getAdjectives(toks):\n",
    "    toks_with_adjectives = []\n",
    "    for tok in toks:\n",
    "        adjs = [left for left in tok.lefts if left.dep_ in ADJECTIVES]\n",
    "        adjs.append(tok)\n",
    "        adjs.extend([right for right in tok.rights if tok.dep_ in ADJECTIVES])\n",
    "        tok_with_adj = \" \".join([adj.lower_ for adj in adjs])\n",
    "        toks_with_adjectives.extend(adjs)\n",
    "\n",
    "    return toks_with_adjectives\n",
    "\n",
    "def getObjsFromAttrs(deps):\n",
    "    for dep in deps:\n",
    "        if dep.pos_ == \"NOUN\" and dep.dep_ == \"attr\":\n",
    "            verbs = [tok for tok in dep.rights if tok.pos_ == \"VERB\"]\n",
    "            if len(verbs) > 0:\n",
    "                for v in verbs:\n",
    "                    rights = list(v.rights)\n",
    "                    objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
    "                    objs.extend(getObjsFromPrepositions(rights))\n",
    "                    if len(objs) > 0:\n",
    "                        return v, objs\n",
    "    return None, None\n",
    "\n",
    "def getObjFromXComp(deps):\n",
    "    for dep in deps:\n",
    "        if dep.pos_ == \"VERB\" and dep.dep_ == \"xcomp\":\n",
    "            v = dep\n",
    "            rights = list(v.rights)\n",
    "            objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
    "            objs.extend(getObjsFromPrepositions(rights))\n",
    "            if len(objs) > 0:\n",
    "                return v, objs\n",
    "    return None, None\n",
    "\n",
    "def getAllSubs(v):\n",
    "    verbNegated = isNegated(v)\n",
    "    subs = [tok for tok in v.lefts if tok.dep_ in SUBJECTS and tok.pos_ != \"DET\"]\n",
    "    if len(subs) > 0:\n",
    "        subs.extend(getSubsFromConjunctions(subs))\n",
    "    else:\n",
    "        foundSubs, verbNegated = findSubs(v)\n",
    "        subs.extend(foundSubs)\n",
    "    return subs, verbNegated\n",
    "\n",
    "def getAllObjs(v):\n",
    "    # rights is a generator\n",
    "    rights = list(v.rights)\n",
    "    objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
    "    objs.extend(getObjsFromPrepositions(rights))\n",
    "\n",
    "    potentialNewVerb, potentialNewObjs = getObjFromXComp(rights)\n",
    "    if potentialNewVerb is not None and potentialNewObjs is not None and len(potentialNewObjs) > 0:\n",
    "        objs.extend(potentialNewObjs)\n",
    "        v = potentialNewVerb\n",
    "    if len(objs) > 0:\n",
    "        objs.extend(getObjsFromConjunctions(objs))\n",
    "    return v, objs\n",
    "\n",
    "def getAllObjsWithAdjectives(v):\n",
    "    # rights is a generator\n",
    "    rights = list(v.rights)\n",
    "    objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
    "\n",
    "    if len(objs)== 0:\n",
    "        objs = [tok for tok in rights if tok.dep_ in ADJECTIVES]\n",
    "\n",
    "    objs.extend(getObjsFromPrepositions(rights))\n",
    "\n",
    "    potentialNewVerb, potentialNewObjs = getObjFromXComp(rights)\n",
    "    if potentialNewVerb is not None and potentialNewObjs is not None and len(potentialNewObjs) > 0:\n",
    "        objs.extend(potentialNewObjs)\n",
    "        v = potentialNewVerb\n",
    "    if len(objs) > 0:\n",
    "        objs.extend(getObjsFromConjunctions(objs))\n",
    "    return v, objs\n",
    "\n",
    "def findSVOs(tokens):\n",
    "    svos = []\n",
    "    verbs = [tok for tok in tokens if tok.pos_ == \"VERB\" and tok.dep_ != \"aux\"]\n",
    "    for v in verbs:\n",
    "        subs, verbNegated = getAllSubs(v)\n",
    "        # hopefully there are subs, if not, don't examine this verb any longer\n",
    "        if len(subs) > 0:\n",
    "            v, objs = getAllObjs(v)\n",
    "            for sub in subs:\n",
    "                for obj in objs:\n",
    "                    objNegated = isNegated(obj)\n",
    "                    svos.append((sub.lower_, \"!\" + v.lower_ if verbNegated or objNegated else v.lower_, obj.lower_))\n",
    "    return svos\n",
    "\n",
    "def findSVAOs(tokens):\n",
    "    svos = []\n",
    "    verbs = [tok for tok in tokens if tok.pos_ == \"VERB\" and tok.dep_ != \"aux\"]\n",
    "    for v in verbs:\n",
    "        subs, verbNegated = getAllSubs(v)\n",
    "        # hopefully there are subs, if not, don't examine this verb any longer\n",
    "        if len(subs) > 0:\n",
    "            v, objs = getAllObjsWithAdjectives(v)\n",
    "            for sub in subs:\n",
    "                for obj in objs:\n",
    "                    objNegated = isNegated(obj)\n",
    "                    obj_desc_tokens = generate_left_right_adjectives(obj)\n",
    "                    sub_compound = generate_sub_compound(sub)\n",
    "                    svos.append((\" \".join(tok.lower_ for tok in sub_compound), \"!\" + v.lower_ if verbNegated or objNegated else v.lower_, \" \".join(tok.lower_ for tok in obj_desc_tokens)))\n",
    "    return svos\n",
    "\n",
    "def generate_sub_compound(sub):\n",
    "    sub_compunds = []\n",
    "    for tok in sub.lefts:\n",
    "        if tok.dep_ in COMPOUNDS:\n",
    "            sub_compunds.extend(generate_sub_compound(tok))\n",
    "    sub_compunds.append(sub)\n",
    "    for tok in sub.rights:\n",
    "        if tok.dep_ in COMPOUNDS:\n",
    "            sub_compunds.extend(generate_sub_compound(tok))\n",
    "    return sub_compunds\n",
    "\n",
    "def generate_left_right_adjectives(obj):\n",
    "    obj_desc_tokens = []\n",
    "    for tok in obj.lefts:\n",
    "        if tok.dep_ in ADJECTIVES:\n",
    "            obj_desc_tokens.extend(generate_left_right_adjectives(tok))\n",
    "    obj_desc_tokens.append(obj)\n",
    "\n",
    "    for tok in obj.rights:\n",
    "        if tok.dep_ in ADJECTIVES:\n",
    "            obj_desc_tokens.extend(generate_left_right_adjectives(tok))\n",
    "\n",
    "    return obj_desc_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data; Create Two Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "News Report     162\n",
       "Social Media     18\n",
       "Fiction           2\n",
       "Name: Origin, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the CSV as a dataframe\n",
    "# colnames = ['Title' , 'Date', 'Author', 'Origin', 'URL', 'Text']\n",
    "df = pandas.read_csv('./clowns_3.csv')\n",
    "# df.shape \n",
    "# df.head()\n",
    "df.Origin.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two lists:\n",
    "news = df[df[\"Origin\"] == \"News Report\"].Text.tolist()\n",
    "social = df[df[\"Origin\"] == \"Social Media\"].Text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An Ohio school district closed schools today after\n"
     ]
    }
   ],
   "source": [
    "print(news[0][0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up our two lists\n",
    "news_strings = [string_test(i) for i in news]\n",
    "social_strings = [string_test(i) for i in social]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminate carriage returns\n",
    "# legends = []\n",
    "# for string in strings:\n",
    "#     string = string.replace(u'\\xa0', u' ')\n",
    "#     legends.append(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish which parser spacy is going to use\n",
    "nlp = spacy.load('en_core_web_sm') # More common is \"en_core_web_sm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_docs = [nlp(i) for i in news_strings]\n",
    "social_docs = [nlp(i) for i in social_strings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I am not saying this is real., I don't know if this is a creepy man or woman or teen., I don't know if this is a person playing a prank., All I'm saying is there are news reports everywhere warning about this., I am simply saying to be careful and be cautious and lookout., That's all.]\n"
     ]
    }
   ],
   "source": [
    "# print([token.text for token in docs[0]])\n",
    "\n",
    "sentences = list(social_docs[0].sents) # spacy's .sents method creates a generator\n",
    "print(sentences[3:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"7f9f5e5ab390496fa3e5df3d2ca12b3a-0\" class=\"displacy\" width=\"2500\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">do</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">n't</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">know</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">if</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">SCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">this</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">creepy</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">man</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">or</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">woman</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">or</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2325\">teen.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2325\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7f9f5e5ab390496fa3e5df3d2ca12b3a-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,2.0 575.0,2.0 575.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7f9f5e5ab390496fa3e5df3d2ca12b3a-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7f9f5e5ab390496fa3e5df3d2ca12b3a-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,89.5 570.0,89.5 570.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7f9f5e5ab390496fa3e5df3d2ca12b3a-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7f9f5e5ab390496fa3e5df3d2ca12b3a-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7f9f5e5ab390496fa3e5df3d2ca12b3a-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">neg</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7f9f5e5ab390496fa3e5df3d2ca12b3a-0-3\" stroke-width=\"2px\" d=\"M770,264.5 C770,89.5 1095.0,89.5 1095.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7f9f5e5ab390496fa3e5df3d2ca12b3a-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">mark</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,266.5 L762,254.5 778,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7f9f5e5ab390496fa3e5df3d2ca12b3a-0-4\" stroke-width=\"2px\" d=\"M945,264.5 C945,177.0 1090.0,177.0 1090.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7f9f5e5ab390496fa3e5df3d2ca12b3a-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,266.5 L937,254.5 953,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7f9f5e5ab390496fa3e5df3d2ca12b3a-0-5\" stroke-width=\"2px\" d=\"M595,264.5 C595,2.0 1100.0,2.0 1100.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7f9f5e5ab390496fa3e5df3d2ca12b3a-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">ccomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1100.0,266.5 L1108.0,254.5 1092.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7f9f5e5ab390496fa3e5df3d2ca12b3a-0-6\" stroke-width=\"2px\" d=\"M1295,264.5 C1295,89.5 1620.0,89.5 1620.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7f9f5e5ab390496fa3e5df3d2ca12b3a-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1295,266.5 L1287,254.5 1303,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7f9f5e5ab390496fa3e5df3d2ca12b3a-0-7\" stroke-width=\"2px\" d=\"M1470,264.5 C1470,177.0 1615.0,177.0 1615.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7f9f5e5ab390496fa3e5df3d2ca12b3a-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1470,266.5 L1462,254.5 1478,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7f9f5e5ab390496fa3e5df3d2ca12b3a-0-8\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,2.0 1625.0,2.0 1625.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7f9f5e5ab390496fa3e5df3d2ca12b3a-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1625.0,266.5 L1633.0,254.5 1617.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7f9f5e5ab390496fa3e5df3d2ca12b3a-0-9\" stroke-width=\"2px\" d=\"M1645,264.5 C1645,177.0 1790.0,177.0 1790.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7f9f5e5ab390496fa3e5df3d2ca12b3a-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1790.0,266.5 L1798.0,254.5 1782.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7f9f5e5ab390496fa3e5df3d2ca12b3a-0-10\" stroke-width=\"2px\" d=\"M1645,264.5 C1645,89.5 1970.0,89.5 1970.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7f9f5e5ab390496fa3e5df3d2ca12b3a-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1970.0,266.5 L1978.0,254.5 1962.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7f9f5e5ab390496fa3e5df3d2ca12b3a-0-11\" stroke-width=\"2px\" d=\"M1995,264.5 C1995,177.0 2140.0,177.0 2140.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7f9f5e5ab390496fa3e5df3d2ca12b3a-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2140.0,266.5 L2148.0,254.5 2132.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7f9f5e5ab390496fa3e5df3d2ca12b3a-0-12\" stroke-width=\"2px\" d=\"M1995,264.5 C1995,89.5 2320.0,89.5 2320.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7f9f5e5ab390496fa3e5df3d2ca12b3a-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2320.0,266.5 L2328.0,254.5 2312.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(sentences[4], style='dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute '_closed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-abf600d27ee8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mPath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"images/sentence-4.svg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/pathlib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0mthe\u001b[0m \u001b[0mbuilt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mdoes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \"\"\"\n\u001b[0;32m-> 1180\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m         return io.open(str(self), mode, buffering, encoding, errors, newline,\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute '_closed'"
     ]
    }
   ],
   "source": [
    "Path.open(\"images/sentence-4.svg\").write(svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'spacy.tokens.token.Token' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-6ffd447aaad5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#     doc = nlp(sentence)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msvg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisplacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dep\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjupyter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'sentence-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".svg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moutput_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"images/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/spacy/displacy/__init__.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(docs, style, page, minify, jupyter, options, manual)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpan\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE096\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'spacy.tokens.token.Token' object is not iterable"
     ]
    }
   ],
   "source": [
    "for sentence in sentences[4]:\n",
    "#     doc = nlp(sentence)\n",
    "    svg = displacy.render(sentence, style=\"dep\", jupyter=False)\n",
    "    file_name = 'sentence-' + \".svg\"\n",
    "    output_path = Path(\"images/\" + file_name)\n",
    "    output_path.open(\"w\", encoding=\"utf-8\").write(svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "# parser = English()\n",
    "parser = spacy.load('en', disable=['ner','textcat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', '!take', 'video'), ('friend', 'sent', 'these'), ('friend', 'sent', 'to'), ('person', 'playing', 'prank')]\n"
     ]
    }
   ],
   "source": [
    "parse = parser(social[0])\n",
    "print(findSVOs(parse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', '!take', 'video'), ('friend', 'sent', 'these'), ('friend', 'sent', 'to'), ('person', 'playing', 'prank')]\n",
      "[('you', 'scare', 'someone'), ('batman', 'had', 'enough'), ('he', 'chasing', 'them'), ('clowns', 'giving', 'kids'), ('clowns', 'giving', 'nights'), ('batman', 'taking', 'care')]\n",
      "[('sightings', 'reported', 'atm')]\n",
      "[('teenager', 'protect', 'age'), ('i', 'call', 'jake'), ('we', 'heard', 'noise'), ('we', 'saw', 'man'), ('we', 'saw', 'knife'), ('he', 'chasing', 'us'), ('clown', 'chasing', 'us'), ('he', 'followed', 'us'), ('i', 'feel', 'him'), ('he', '!see', 'us'), ('he', 'left', 'bush'), ('we', 'ran', 'pack'), ('we', '!tell', 'them'), ('i', 'said', 'goodbye')]\n",
      "[('weapon', 'scare', 'clowns'), ('i', 'saw', 'clowns'), ('i', 'introduce', 'you'), ('guys', 'called', 'smith'), ('guys', 'called', 'goldreply'), ('bencoffee', 'get', 'shotgun'), ('you', 'shoot', 'them'), ('you', 'shoot', 'goldreply'), ('i', 'prefer', 'girl'), ('you', 'hold', 'clowns'), ('you', 'fucking', 'pussies')]\n",
      "[('advice', 'bring', 'case'), ('we', 'encounter', 'clown'), ('advice', 'bring(except', 'those'), ('who', 'bring', 'lubricant'), ('who', 'bring', 'condoms'), ('advice', 'bring', 'lubricant'), ('advice', 'bring', 'condoms'), ('goldreply', 'bring', 'kit'), ('you', 'skin', 'clown'), ('goldreply', 'skin', 'clown'), ('goldreply', 'keep', 'pelt'), ('you', 'hunt', 'clowns'), ('you', '!hunt', 'clowns'), ('you', 'collect', 'pelts')]\n",
      "[('districts', 'announced', 'ban'), ('school', 'banned', 'form'), ('one', 'wore', 'coats'), ('they', 'kept', 'classrooms'), ('school', 'makes', 'money'), ('they', 'banned', 'uniform'), ('who', 'using', 'hand'), ('you', 'shitting', 'me'), ('they', 'played', 'ruit'), ('i', 'wore', 'duster'), ('one', 'gave', 'shit'), ('i', 'made', 'decisions'), ('you', 'stating', 'age'), ('we', 'learned', 'everything'), ('people', 'wore', 'them'), ('thing', 'annoyed', 'me'), ('who', 'wearing', 'coats'), ('it', 'seems', 'fair'), ('kids', 'wearing', 'jerseys'), ('we', 'had', 'policy'), ('everyone', 'seems', 'goldreply'), ('they', 'banned', 'outerwear'), ('i', 'pointed', 'fact'), ('administration', 'wore', 'suits'), ('they', 'changed', 'rule'), ('students', 'wear', 'suits'), ('everyone', 'wear', 'underwear'), ('we', 'call', 'clothes'), ('coat', 'conceal', 'weapon'), ('school', 'force', 'us'), ('we', '!carry', 'backpacks'), ('everyone', 'using', 'bags'), ('everyone', 'using', 'bags'), ('system', 'go', 'duck'), ('me', 'use', 'locker'), ('everyone', 'used', 'lockers'), ('everyone', 'used', 'packs'), ('i', '!imagine', 'life'), ('i', '!imagine', 'goldreply'), ('room', 'fit', 'books'), ('room', 'fit', 'binders'), ('time', 'use', 'locker'), ('you', 'run', 'risk'), ('risk', 'missing', 'bus'), ('risk', 'find', 'way'), ('risk', 'find', 'way'), ('day', 'eliminated', 'possibility'), ('possibility', 'forgetting', 'things'), ('i', 'cart', 'books'), ('i', 'leave', 'books'), ('shoulders', 'hurt', 'bit'), ('goldreply', 'continue', 'thread'), ('you', 'had', 'school'), ('goldreply', 'continue', 'thread'), ('people', '!wearing', 'backpacks'), ('people', '!wearing', 'coats'), ('we', '!carry', 'bag'), ('we', 'ban', 'pants'), ('you', 'fit', 'knife'), ('it', 'glorified', 'piece'), ('i', 'm', 'hate'), ('ability', 'conceal', 'shottie'), ('you', 'put', 'legit'), ('you', 'conceal', 'weapon'), ('it', 'seems', 'fair'), ('who', 'give', 'scions'), ('who', 'give', 'name'), ('tool', 'buy', 'one'), ('son', 'kill', 'people'), ('i', 'remember', 'that'), ('kids', 'fucked', 'it'), ('people', 'wore', 'them'), ('school', 'banned', 'sort'), ('dying', 'makes', 'difference'), ('one', 'makes', 'difference'), ('school', 'received', 'threats'), ('costume', 'outlawed', 'goldreply'), ('i', 'wore', 'coats'), ('everyone', 'called', 'me'), ('everyone', 'called', 'mafia'), ('i', 'wore', 'crap'), ('video', 'committing', 'robberies'), ('people', 'posting', 'pictures'), ('people', 'posting', 'videos'), ('you', 'mean', 'hoax'), ('clowns', 'lure', 'children'), ('clowns', 'lure', 'children'), ('clowns', 'lure', 'children'), ('person', 'made', 'report'), ('person', 'made', 'nothing'), ('i', 'make', 'report'), ('hoffa', 'assaulted', 'me'), ('you', 'mean', 'hoax'), ('clowns', 'lure', 'children'), ('clowns', 'lure', 'children'), ('needs', 'trolling', 'people'), ('mask', 'hides', 'identity'), ('nobody', 'produced', 'video'), ('movie', '!entered', 'production'), ('them', 'doing', 'it'), ('witch', 'started', 'marketing'), ('marketing', 'considered', 'success'), ('i', '!put', 'it'), ('studio', 'continuing', 'marketing'), ('zombie', 'doing', 'movie'), ('movie', 'called', '31'), ('i', 'promise', 'goldreply'), ('disappointment', 'abounds', 'goldreply'), ('rejects', 'gave', 'me'), ('rejects', 'gave', 'hope'), ('i', 'see', 'more'), ('i', 'told', 'wife'), ('bridge', 'chasing', 'guys'), ('video', 'committing', 'robberies'), ('people', 'posting', 'pictures'), ('people', 'posting', 'videos'), ('you', 'mean', 'hoax'), ('clowns', 'lure', 'children'), ('clowns', 'lure', 'children'), ('clowns', 'lure', 'children'), ('person', 'made', 'report'), ('person', 'made', 'nothing'), ('i', 'make', 'report'), ('hoffa', 'assaulted', 'me'), ('you', 'mean', 'hoax'), ('clowns', 'lure', 'children'), ('clowns', 'lure', 'children'), ('needs', 'trolling', 'people'), ('mask', 'hides', 'identity'), ('nobody', 'produced', 'video'), ('movie', '!entered', 'production'), ('them', 'doing', 'it'), ('witch', 'started', 'marketing'), ('marketing', 'considered', 'success'), ('i', '!put', 'it'), ('studio', 'continuing', 'marketing'), ('zombie', 'doing', 'movie'), ('movie', 'called', '31'), ('i', 'promise', 'goldreply'), ('disappointment', 'abounds', 'goldreply'), ('rejects', 'gave', 'me'), ('rejects', 'gave', 'hope'), ('i', 'see', 'more'), ('i', 'told', 'wife'), ('bridge', 'chasing', 'guys'), ('i', 'shipping', 'them'), ('trenchcoat', 'wearing', 'kids'), ('i', '!wear', 'mine'), ('it', 'convince', 'years'), ('it', 'convince', 'people'), ('kids', '!afford', 'replacements'), ('people', 'buying', 'backpacks'), ('they', 'banned', 'costumes'), ('people', 'shoot', 'them'), ('them', 'doing', 'something'), ('others', 'doing', 'something'), ('others', 'doing', 'to'), ('we', 'seen', 'people'), ('they', 'attack', 'anyone'), ('clowns', 'lure', 'children'), ('clowns', 'lure', 'children'), ('mask', 'hides', 'identity'), ('clowns', 'lure', 'children'), ('clowns', 'lure', 'children'), ('nobody', 'trying', 'that'), ('people', 'doing', 'it'), ('teenager', 'scaring', 'people'), ('he', 'warned', 'him'), ('it', 'scaring', 'kid'), ('they', 'made', 'way'), ('they', 'find', 'it'), ('girls', 'take', 'pictures')]\n",
      "[('clown', 'chased', 'kids'), ('i', 'get', 'it'), ('it', 'seem', 'funny'), ('fathers', '!put', 'you'), ('you', 'threaten', 'children'), ('i', 'understand', 'pranks'), ('i', 'knew', 'identity'), ('clown', 'running', 'me'), ('i', 'kick', 'him'), ('you', '!know', 'that'), ('we', '!carry', 'spray'), ('easier', 'find', 'machete'), ('machete', 'find', 'one'), ('she', 'called', 'it'), ('she', 'called', 'knife'), ('she', 'called', 'size'), ('we', '!carry', 'spray'), ('it', 'contain', 'bears'), ('you', 'seen', 'wasps'), ('someone', 'kill', 'someone'), ('i', 'find', 'thing'), ('people', 'take', 'it'), ('tip', 'painted', 'black'), ('kid', 'got', 'charges'), ('tsahandjobs', 'using', 'weapon'), ('tsahandjobs', 'carries', 'punishments'), ('you', 'committing', 'crime'), ('i', 'understand', 'impulse'), ('they', 'get', 'douchey'), ('i', 'hurt', 'people'), ('i', '!gave', 'clue'), ('i', 'laughed', 'thinking'), ('clowns', 'disrupting', 'peace'), ('who', '!put', 'much'), ('clowns', 'disrupting', 'peace'), ('i', 'read', 'sign'), ('father', 'given', 'to'), ('katanas', 'given', 'to'), ('i', 'see', 'anyone'), ('i', '!run', 'him'), ('police', 'called', 'whilst'), ('they', 'wielding', 'weapons'), ('anyone', 'chasing', 'children'), ('i', 'run', 'fucker'), ('playing', 'make', 'belief'), ('clown', 'chasing', 'you'), ('father', 'take', 'bat'), ('clown', 'chasing', 'child'), ('you', 'spend', 'years'), ('one', 'considers', 'that'), ('clown', 'chasing', 'kids'), ('they', 'defend', 'themselves'), ('i', 'run', 'them'), ('law', 'allows', 'use'), ('use', 'defending', 'life'), ('clown', 'murder', 'kids'), ('mowing', 'see', 'charges'), ('you', 'see', 'jail'), ('someone', 'make', 'this'), ('clown', 'grafittid', 'school'), ('fuckers', 'doing', 'that'), ('i', 'spraypaint', 'smiles')]\n",
      "[('i', 'saw', 'one'), ('it', 'wearing', 'shirt'), ('i', '!see', 'eyes'), ('it', 'staring', 'me'), ('i', 'snapped', 'pic'), ('you', 'blew', 'chance'), ('you', 'blew', 'goldreply'), ('pic', 'horrifying', 'goldreply'), ('i', 'threw', 'head'), ('you', 'got', 'me'), ('reason', 'kill', 'them')]\n",
      "[('guys', 'joined', 'reddit'), ('lurker', 'joined', 'reddit'), ('anyone', 'heard', 'stuff'), ('clowns', 'killing', 'children'), ('you', 'see', 'clown'), ('clowns', 'get', 'taste'), ('they', '!forget', 'it'), ('they', 'kill', 'people'), ('you', 'watch', 'back')]\n",
      "[('one', 'shoot', 'people'), ('someone', 'chases', 'me'), ('i', 'shoot', 'them'), ('they', 'attack', 'me'), ('someone', 'run', 'me'), ('i', 'shot', 'them'), ('i', 'shot', 'goldreply'), ('i', '!like', 'clowns'), ('we', 'hunt', 'clowns'), ('i', 'give', 'shit'), ('you', 'bring', 'aks'), ('fuggin', 'shot', 'guns'), ('fuggin', 'shot', 'niggaz'), ('we', 'go', 'rise'), ('you', 'join', 'reply')]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', 'remember', 'everything'), ('i', 'got', 'invitation'), ('i', 'spend', 'night'), ('i', 'made', 'sure'), ('i', 'head', 'road'), ('things', 'got', 'creepier'), ('he', 'gave', 'me'), ('he', 'gave', 'creepy'), ('friend', 'met', 'me'), ('stuff', 'happening', 'lot'), ('i', 'call', 'friend'), ('i', 'asked', 'him'), ('you', 'expecting', 'anyone'), ('plans', 'cancelled', 'all'), ('parents', 'called', 'me'), ('i', 'telling', 'you'), ('i', 'telling', 'i'), ('i', 'saw', 'thing'), ('sign', 'read', 'candy'), ('whoever', 'follows', 'me'), ('henry', 'saw', 'him'), ('he', 'called', 'parents'), ('parents', 'lock', 'door'), ('henry', 'hung', 'phone'), ('i', 'gave', 'him'), ('i', 'gave', 'nod'), ('me', 'clean', 'place'), ('day', 'helping', 'him'), ('i', 'saw', 'sign'), ('he', 'left', 'sign'), ('we', 'locked', 'everything'), ('thought', 'gave', 'us'), ('thought', 'gave', 'chills'), ('henry', 'asked', 'me'), ('he', 'thanked', 'me'), ('i', 'told', 'henry'), ('i', 'told', 'him'), ('i', 'asked', 'him'), ('i', 'called', 'cops'), ('he', 'said', 'this'), ('clown', 'smothering', 'faces'), ('they', 'said', 'that'), ('police', 'brought', 'clown'), ('clown', 'looked', 'me'), ('i', 'felt', 'shrivels'), ('he', 'watching', 'us')]\n",
      "[('they', 'receiving', 'threats'), ('they', 'found', '4'), ('friend', 'saw', 'one'), ('rn', 'excuse', 'grammar'), ('inquisitor_aid', '!squeak', 'nose'), ('inquisitor_aid', '!squeak', 'goldreply')]\n",
      "[('video', 'cut', 'goldreply'), ('i', 'found', 'clowns'), ('bobb_grayy', 'shut', 'pussy'), ('i', 'see', 'clown'), ('i', 'seen', 'one'), ('i', 'tear', 'nose'), ('i', 'have', 'it')]\n",
      "[('they', 'strike', 'world'), ('people', 'feel', 'them'), ('people', 'feel', 'power'), ('cult', 'feel', 'power'), ('i', 'fucking', 'clowns'), ('shit', 'hit', 'fan'), ('people', 'beating', 'clowns'), ('i', 'lock', 'doors'), ('you', 'find', 'halloween')]\n",
      "[('anyone', 'believe', 'me'), ('hill', 'told', 'me'), ('one', 'seen', 'clown')]\n",
      "[('he', 'used', 'treats'), ('attempt', 'lure', 'children'), ('0', 'replies', 'retweet'), ('stories', '!ring', 'me')]\n",
      "[('they', 'use', 'clowns')]\n"
     ]
    }
   ],
   "source": [
    "for item in social:\n",
    "    parse = parser(item)\n",
    "    print(findSVOs(parse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
