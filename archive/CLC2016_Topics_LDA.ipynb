{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clown Legend Cascade of 2016: Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With thanks to [Usman Malik][].\n",
    "\n",
    "[Usman Malik]: https://stackabuse.com/python-for-nlp-topic-modeling/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =-=-=-=-=-=-=-=-=-=-=\n",
    "# IMPORTS & FUNCTIONS\n",
    "# =-=-=-=-=-=-=-=-=-=-= \n",
    "\n",
    "import pandas, re\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "def string_test(s):\n",
    "    if s is None:\n",
    "        return ''\n",
    "    else:\n",
    "        return str(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =-=-=-=-=-=-=-=-=-=-=\n",
    "# DATA\n",
    "# =-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "df = pandas.read_csv('./clowns_3.csv')\n",
    "\n",
    "all_texts = df.Text.tolist()\n",
    "news = df[df[\"Origin\"] == \"News Report\"].Origin.tolist()\n",
    "social = df[df[\"Origin\"] == \"Social Media\"].Origin.tolist()\n",
    "\n",
    "# Check for string (in case of nan)\n",
    "all_strings = [ string_test(text) for text in texts ]\n",
    "news_strings = [string_test(i) for i in news]\n",
    "social_strings = [string_test(i) for i in social]\n",
    "\n",
    "# Eliminate carriage returns\n",
    "legends = []\n",
    "for string in strings:\n",
    "    string = string.replace(u'\\xa0', u' ')\n",
    "    legends.append(string)\n",
    "\n",
    "# # TOKENIZE -- not needed for Sci-Kit Learn\n",
    "# tokenizer = WhitespaceTokenizer()\n",
    "# tokenized = []\n",
    "# for legend in legends:   \n",
    "#     raw = re.sub(r\"[^\\w\\d'\\s]+\",'', legend).lower()\n",
    "#     tokens = tokenizer.tokenize(raw)\n",
    "#     tokenized.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(min_df=2, stop_words='english')\n",
    "doc_term_matrix = count_vect.fit_transform(legends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<182x4032 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 31101 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=10, n_jobs=None, n_topics=None, perp_tol=0.1,\n",
       "             random_state=42, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA = LatentDirichletAllocation(n_components=10, random_state=42)\n",
    "LDA.fit(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for topic #0:\n",
      "['wasn', 'going', 'let', 'said', 'don', 'time', 'didn', 'like', 'clown', 'just']\n",
      "\n",
      "\n",
      "Top 10 words for topic #1:\n",
      "['people', 'students', 'threats', 'high', 'county', 'police', 'clowns', 'said', 'school', 'clown']\n",
      "\n",
      "\n",
      "Top 10 words for topic #2:\n",
      "['people', 'threat', 'schools', 'reports', 'threats', 'clowns', 'clown', 'said', 'school', 'police']\n",
      "\n",
      "\n",
      "Top 10 words for topic #3:\n",
      "['carolina', 'reported', 'told', 'greenville', 'children', 'woods', 'clowns', 'said', 'police', 'clown']\n",
      "\n",
      "\n",
      "Top 10 words for topic #4:\n",
      "['urging', 'rn', 'yo', 'firearm', 'precautions', 'minds', 'responsibility', 'lookout', 'suburb', 'learning']\n",
      "\n",
      "\n",
      "Top 10 words for topic #5:\n",
      "['children', 'year', 'sightings', 'says', 'reports', 'halloween', 'people', 'creepy', 'clowns', 'clown']\n",
      "\n",
      "\n",
      "Top 10 words for topic #6:\n",
      "['media', 'threats', 'like', 'police', 'sightings', 'creepy', 'said', 'people', 'clowns', 'clown']\n",
      "\n",
      "\n",
      "Top 10 words for topic #7:\n",
      "['people', 'permalinkembedsavereportgive', 'just', 'point', 'clown', 'permalinkembedsaveparentreportgive', 'points', 'goldreply', 'months', 'ago']\n",
      "\n",
      "\n",
      "Top 10 words for topic #8:\n",
      "['students', 'penn', 'state', 'sightings', 'said', 'reports', 'people', 'police', 'clowns', 'clown']\n",
      "\n",
      "\n",
      "Top 10 words for topic #9:\n",
      "['juggalos', 've', 'like', 'time', 'creepy', 'percent', 'clown', 'americans', 'afraid', 'clowns']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,topic in enumerate(LDA.components_):\n",
    "    print(f'Top 10 words for topic #{i}:')\n",
    "    print([count_vect.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(182, 10)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_values = LDA.transform(doc_term_matrix)\n",
    "topic_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Topic'] = topic_values.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Author</th>\n",
       "      <th>Origin</th>\n",
       "      <th>URL</th>\n",
       "      <th>Text</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Clown Attack on Woman Forces Cincinnati Suburb...</td>\n",
       "      <td>September 30 2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>News Report</td>\n",
       "      <td>http://insider.foxnews.com/2016/09/30/clown-at...</td>\n",
       "      <td>An Ohio school district closed schools today a...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Another Clown Was Spotted In The Woods And Pol...</td>\n",
       "      <td>September 6 2016</td>\n",
       "      <td>Michelle Broder Van Dyke</td>\n",
       "      <td>News Report</td>\n",
       "      <td>https://www.buzzfeed.com/mbvd/stop-clowning-ar...</td>\n",
       "      <td>The latest clown to be spotted was chased back...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Everyone in poor moiuntain please stay inside....</td>\n",
       "      <td>September 13 2016</td>\n",
       "      <td>Melissa Dooley</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>https://www.facebook.com/melissa.dooley.397/po...</td>\n",
       "      <td>I don't know if this is real or fake. I didn't...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>He's the hero this country deserves</td>\n",
       "      <td>October 13 2016</td>\n",
       "      <td>The LAD Bible</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>https://www.facebook.com/LADbible/videos/29391...</td>\n",
       "      <td>Batman, \"As for you clowns, if you want to sca...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>After-dark clown sightings trouble California ...</td>\n",
       "      <td>October 13 2014</td>\n",
       "      <td>The Associated Press , WBIR</td>\n",
       "      <td>News Report</td>\n",
       "      <td>http://www.wbir.com/news/after-dark-clown-sigh...</td>\n",
       "      <td>BAKERSFIELD, California (AP) â€” People dressed ...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The tears of a (real) clown: All the insane cl...</td>\n",
       "      <td>October 30 2016</td>\n",
       "      <td>Ellen McCarthy</td>\n",
       "      <td>News Report</td>\n",
       "      <td>https://www.washingtonpost.com/lifestyle/the-t...</td>\n",
       "      <td>Listen, punks. You think you can slap on some ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Target halts clown-mask sales amid creepy-clow...</td>\n",
       "      <td>October 17 2016</td>\n",
       "      <td>Amy B Wang</td>\n",
       "      <td>News Report</td>\n",
       "      <td>https://www.washingtonpost.com/news/arts-and-e...</td>\n",
       "      <td>Send out the clowns.\\n\\nTarget will stop selli...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Creepy clowns spook a country already freaked ...</td>\n",
       "      <td>September 29 2016</td>\n",
       "      <td>Petula Dvorak</td>\n",
       "      <td>News Report</td>\n",
       "      <td>https://www.washingtonpost.com/local/creepy-cl...</td>\n",
       "      <td>Turns out that people with a lot of face makeu...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>No laughing matter: Clown sightings have polic...</td>\n",
       "      <td>September 24 2016</td>\n",
       "      <td>WPCO Staff</td>\n",
       "      <td>News Report</td>\n",
       "      <td>http://www.wcpo.com/news/state/state-kentucky/...</td>\n",
       "      <td>Your worst nightmare may be coming true in Ken...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Clown sightings creep into southern Kentucky</td>\n",
       "      <td>September 19 2016</td>\n",
       "      <td>WKYT Staff</td>\n",
       "      <td>News Report</td>\n",
       "      <td>http://www.wkyt.com/content/news/Clown-sightin...</td>\n",
       "      <td>LONDON, Ky. (WKYT) -Â After several sighting in...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title               Date  \\\n",
       "0  Clown Attack on Woman Forces Cincinnati Suburb...  September 30 2016   \n",
       "1  Another Clown Was Spotted In The Woods And Pol...   September 6 2016   \n",
       "2  Everyone in poor moiuntain please stay inside....  September 13 2016   \n",
       "3                He's the hero this country deserves    October 13 2016   \n",
       "4  After-dark clown sightings trouble California ...    October 13 2014   \n",
       "5  The tears of a (real) clown: All the insane cl...    October 30 2016   \n",
       "6  Target halts clown-mask sales amid creepy-clow...    October 17 2016   \n",
       "7  Creepy clowns spook a country already freaked ...  September 29 2016   \n",
       "8  No laughing matter: Clown sightings have polic...  September 24 2016   \n",
       "9       Clown sightings creep into southern Kentucky  September 19 2016   \n",
       "\n",
       "                        Author       Origin  \\\n",
       "0                          NaN  News Report   \n",
       "1     Michelle Broder Van Dyke  News Report   \n",
       "2               Melissa Dooley     Facebook   \n",
       "3                The LAD Bible     Facebook   \n",
       "4  The Associated Press , WBIR  News Report   \n",
       "5               Ellen McCarthy  News Report   \n",
       "6                   Amy B Wang  News Report   \n",
       "7                Petula Dvorak  News Report   \n",
       "8                   WPCO Staff  News Report   \n",
       "9                   WKYT Staff  News Report   \n",
       "\n",
       "                                                 URL  \\\n",
       "0  http://insider.foxnews.com/2016/09/30/clown-at...   \n",
       "1  https://www.buzzfeed.com/mbvd/stop-clowning-ar...   \n",
       "2  https://www.facebook.com/melissa.dooley.397/po...   \n",
       "3  https://www.facebook.com/LADbible/videos/29391...   \n",
       "4  http://www.wbir.com/news/after-dark-clown-sigh...   \n",
       "5  https://www.washingtonpost.com/lifestyle/the-t...   \n",
       "6  https://www.washingtonpost.com/news/arts-and-e...   \n",
       "7  https://www.washingtonpost.com/local/creepy-cl...   \n",
       "8  http://www.wcpo.com/news/state/state-kentucky/...   \n",
       "9  http://www.wkyt.com/content/news/Clown-sightin...   \n",
       "\n",
       "                                                Text  Topic  \n",
       "0  An Ohio school district closed schools today a...      2  \n",
       "1  The latest clown to be spotted was chased back...      3  \n",
       "2  I don't know if this is real or fake. I didn't...      7  \n",
       "3  Batman, \"As for you clowns, if you want to sca...      0  \n",
       "4  BAKERSFIELD, California (AP) â€” People dressed ...      8  \n",
       "5  Listen, punks. You think you can slap on some ...      6  \n",
       "6  Send out the clowns.\\n\\nTarget will stop selli...      6  \n",
       "7  Turns out that people with a lot of face makeu...      6  \n",
       "8  Your worst nightmare may be coming true in Ken...      8  \n",
       "9  LONDON, Ky. (WKYT) -Â After several sighting in...      5  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to call the texts straight from the dataframe, with `df['Text'].values.astype('U')` replacing `legends` as in:\n",
    "\n",
    "```\n",
    "doc_term_matrix = tfidf_vect.fit_transform(df['Text'].values.astype('U'))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(min_df=2, stop_words='english')\n",
    "doc_term_matrix = tfidf_vect.fit_transform(legends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
       "  n_components=10, random_state=None, shuffle=False, solver='cd',\n",
       "  tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf = NMF(n_components=10)\n",
    "nmf.fit(doc_term_matrix )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words per topic:\n",
      "\n",
      "#0: ['sightings', 'king', 'scary', 'like', 'hysteria', 'says', 'creepy', 'people', 'clowns', 'clown']\n",
      "#1: ['clown', 'clowns', 'just', 'point', 'permalinkembedsaveparentreportgive', 'permalinkembedsavereportgive', 'points', 'ago', 'months', 'goldreply']\n",
      "#2: ['apartments', 'said', 'complex', 'children', 'manor', 'clowns', 'apartment', 'fleetwood', 'woods', 'greenville']\n",
      "#3: ['district', 'said', 'clown', 'students', 'high', 'police', 'threat', 'schools', 'threats', 'school']\n",
      "#4: ['hunt', 'twitter', 'clowns', 'college', 'campus', 'clown', 'nelson', 'students', 'state', 'penn']\n",
      "#5: ['winston', 'salem', 'red', 'north', 'children', 'woods', 'greensboro', 'police', 'carolina', 'clown']\n",
      "#6: ['think', 'jamie', 'ky', 'month', 'deputies', 'clown', 'picture', 'didn', 'hill', 'london']\n",
      "#7: ['know', 'didn', 'dude', 'don', 'fucking', 'went', 'house', 'just', 'said', 'henry']\n",
      "#8: ['boy', 'incident', 'columbus', 'reports', 'tuesday', 'wearing', 'mask', 'clown', 'said', 'police']\n",
      "#9: ['threats', 'matter', 'lives', 'event', 'said', 'flier', 'arizona', 'tucson', 'march', 'clown']\n"
     ]
    }
   ],
   "source": [
    "print(f'Top 10 words per topic:\\n')\n",
    "for i,topic in enumerate(nmf.components_):\n",
    "    print(f'#{i}:', end=\" \")\n",
    "    print([tfidf_vect.get_feature_names()[i] for i in topic.argsort()[-10:]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
