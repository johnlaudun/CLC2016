{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas, spacy, textacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data; Create Two Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV as a dataframe\n",
    "colnames = ['Title' , 'Date', 'Author', 'Origin', 'URL', 'Text']\n",
    "df = pandas.read_csv('./clowns_2a.csv', names=colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two lists (because I'm not so good on working with the dataframe)\n",
    "texts = df.Text.tolist()\n",
    "dates = df.Date.tolist()\n",
    "\n",
    "def string_test(s):\n",
    "    if s is None:\n",
    "        return ''\n",
    "    else:\n",
    "        return str(s)\n",
    "\n",
    "# With any luck, this list comprehension will work:\n",
    "strings = [ string_test(text) for text in texts ]\n",
    "\n",
    "# Eliminate carriage returns\n",
    "legends = []\n",
    "for string in strings:\n",
    "    string = string.replace(u'\\xa0', u' ')\n",
    "    legends.append(string)\n",
    "\n",
    "# Establish which parser spacy is going to use\n",
    "nlp = spacy.load('en') # More common is \"en_core_web_sm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This produces a list of SpaCy documents, \n",
    "# not the one big doc that SpaCy appears to expect.\n",
    "docs = [nlp(legend) for legend in legends]\n",
    "type(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print([token.text for token in docs[0]])\n",
    "\n",
    "sentences = list(docs[0].sents) # spacy's .sents method creates a generator\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(sentences[0], style='dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtree_matcher(doc): \n",
    "    x = '' \n",
    "    y = '' \n",
    "  \n",
    "    # iterate through all the tokens in the input sentence \n",
    "    for i,tok in enumerate(doc): \n",
    "        # extract subject \n",
    "        if tok.dep_.find(\"subjpass\") == True: \n",
    "            y = tok.text \n",
    "        # extract object \n",
    "        if tok.dep_.endswith(\"obj\") == True: \n",
    "            x = tok.text \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtree_matcher(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stackoverflow Solutions\n",
    "\n",
    "Both of the solutions below were in response to [How to extract subjects in a sentence and their respective dependent phrases?](https://stackoverflow.com/questions/39763091/how-to-extract-subjects-in-a-sentence-and-their-respective-dependent-phrases)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Krzysiek\n",
    "\n",
    "**>>> Currently this solution does not work, but I am holding it here as an interesting possible solution to explore.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u\"The Empire of Japan aimed to dominate Asia and the \" \\\n",
    "               \"Pacific and was already at war with the Republic of China \" \\\n",
    "               \"in 1937, but the world war is generally said to have begun on \" \\\n",
    "               \"1 September 1939 with the invasion of Poland by Germany and \" \\\n",
    "               \"subsequent declarations of war on Germany by France and the United Kingdom. \" \\\n",
    "               \"From late 1939 to early 1941, in a series of campaigns and treaties, Germany conquered \" \\\n",
    "               \"or controlled much of continental Europe, and formed the Axis alliance with Italy and Japan. \" \\\n",
    "               \"Under the Molotov-Ribbentrop Pact of August 1939, Germany and the Soviet Union partitioned and \" \\\n",
    "               \"annexed territories of their European neighbours, Poland, Finland, Romania and the Baltic states. \" \\\n",
    "               \"The war continued primarily between the European Axis powers and the coalition of the United Kingdom \" \\\n",
    "               \"and the British Commonwealth, with campaigns including the North Africa and East Africa campaigns, \" \\\n",
    "               \"the aerial Battle of Britain, the Blitz bombing campaign, the Balkan Campaign as well as the \" \\\n",
    "               \"long-running Battle of the Atlantic. In June 1941, the European Axis powers launched an invasion \" \\\n",
    "               \"of the Soviet Union, opening the largest land theatre of war in history, which trapped the major part \" \\\n",
    "               \"of the Axis' military forces into a war of attrition. In December 1941, Japan attacked \" \\\n",
    "               \"the United States and European territories in the Pacific Ocean, and quickly conquered much of \" \\\n",
    "               \"the Western Pacific.\")\n",
    "\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textpipeliner import PipelineEngine, Context\n",
    "from textpipeliner.pipes import *\n",
    "\n",
    "import spacy\n",
    "from textpipeliner import PipelineEngine, Context\n",
    "from textpipeliner.pipes import *\n",
    "\n",
    "pipes_structure = [SequencePipe([FindTokensPipe(\"VERB/nsubj/NNP\"),\n",
    "                                 NamedEntityFilterPipe(),\n",
    "                                 NamedEntityExtractorPipe()]),\n",
    "                       AggregatePipe([FindTokensPipe(\"VERB\"),\n",
    "                                      FindTokensPipe(\"VERB/xcomp/VERB/aux/*\"),\n",
    "                                      FindTokensPipe(\"VERB/xcomp/VERB\")]),\n",
    "                       AnyPipe([FindTokensPipe(\"VERB/[acomp,amod]/ADJ\"),\n",
    "                                AggregatePipe([FindTokensPipe(\"VERB/[dobj,attr]/NOUN/det/DET\"),\n",
    "                                               FindTokensPipe(\"VERB/[dobj,attr]/NOUN/[acomp,amod]/ADJ\")])])\n",
    "                      ]\n",
    "\n",
    "engine = PipelineEngine(pipes_structure, Context(doc), [0,1,2])\n",
    "engine.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## psr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBJECTS = [\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"]\n",
    "OBJECTS = [\"dobj\", \"dative\", \"attr\", \"oprd\"]\n",
    "ADJECTIVES = [\"acomp\", \"advcl\", \"advmod\", \"amod\", \"appos\", \"nn\", \"nmod\", \"ccomp\", \"complm\",\n",
    "              \"hmod\", \"infmod\", \"xcomp\", \"rcmod\", \"poss\",\" possessive\"]\n",
    "COMPOUNDS = [\"compound\"]\n",
    "PREPOSITIONS = [\"prep\"]\n",
    "\n",
    "def getSubsFromConjunctions(subs):\n",
    "    moreSubs = []\n",
    "    for sub in subs:\n",
    "        # rights is a generator\n",
    "        rights = list(sub.rights)\n",
    "        rightDeps = {tok.lower_ for tok in rights}\n",
    "        if \"and\" in rightDeps:\n",
    "            moreSubs.extend([tok for tok in rights if tok.dep_ in SUBJECTS or tok.pos_ == \"NOUN\"])\n",
    "            if len(moreSubs) > 0:\n",
    "                moreSubs.extend(getSubsFromConjunctions(moreSubs))\n",
    "    return moreSubs\n",
    "\n",
    "def getObjsFromConjunctions(objs):\n",
    "    moreObjs = []\n",
    "    for obj in objs:\n",
    "        # rights is a generator\n",
    "        rights = list(obj.rights)\n",
    "        rightDeps = {tok.lower_ for tok in rights}\n",
    "        if \"and\" in rightDeps:\n",
    "            moreObjs.extend([tok for tok in rights if tok.dep_ in OBJECTS or tok.pos_ == \"NOUN\"])\n",
    "            if len(moreObjs) > 0:\n",
    "                moreObjs.extend(getObjsFromConjunctions(moreObjs))\n",
    "    return moreObjs\n",
    "\n",
    "def getVerbsFromConjunctions(verbs):\n",
    "    moreVerbs = []\n",
    "    for verb in verbs:\n",
    "        rightDeps = {tok.lower_ for tok in verb.rights}\n",
    "        if \"and\" in rightDeps:\n",
    "            moreVerbs.extend([tok for tok in verb.rights if tok.pos_ == \"VERB\"])\n",
    "            if len(moreVerbs) > 0:\n",
    "                moreVerbs.extend(getVerbsFromConjunctions(moreVerbs))\n",
    "    return moreVerbs\n",
    "\n",
    "def findSubs(tok):\n",
    "    head = tok.head\n",
    "    while head.pos_ != \"VERB\" and head.pos_ != \"NOUN\" and head.head != head:\n",
    "        head = head.head\n",
    "    if head.pos_ == \"VERB\":\n",
    "        subs = [tok for tok in head.lefts if tok.dep_ == \"SUB\"]\n",
    "        if len(subs) > 0:\n",
    "            verbNegated = isNegated(head)\n",
    "            subs.extend(getSubsFromConjunctions(subs))\n",
    "            return subs, verbNegated\n",
    "        elif head.head != head:\n",
    "            return findSubs(head)\n",
    "    elif head.pos_ == \"NOUN\":\n",
    "        return [head], isNegated(tok)\n",
    "    return [], False\n",
    "\n",
    "def isNegated(tok):\n",
    "    negations = {\"no\", \"not\", \"n't\", \"never\", \"none\"}\n",
    "    for dep in list(tok.lefts) + list(tok.rights):\n",
    "        if dep.lower_ in negations:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def findSVs(tokens):\n",
    "    svs = []\n",
    "    verbs = [tok for tok in tokens if tok.pos_ == \"VERB\"]\n",
    "    for v in verbs:\n",
    "        subs, verbNegated = getAllSubs(v)\n",
    "        if len(subs) > 0:\n",
    "            for sub in subs:\n",
    "                svs.append((sub.orth_, \"!\" + v.orth_ if verbNegated else v.orth_))\n",
    "    return svs\n",
    "\n",
    "def getObjsFromPrepositions(deps):\n",
    "    objs = []\n",
    "    for dep in deps:\n",
    "        if dep.pos_ == \"ADP\" and dep.dep_ == \"prep\":\n",
    "            objs.extend([tok for tok in dep.rights if tok.dep_  in OBJECTS or (tok.pos_ == \"PRON\" and tok.lower_ == \"me\")])\n",
    "    return objs\n",
    "\n",
    "def getAdjectives(toks):\n",
    "    toks_with_adjectives = []\n",
    "    for tok in toks:\n",
    "        adjs = [left for left in tok.lefts if left.dep_ in ADJECTIVES]\n",
    "        adjs.append(tok)\n",
    "        adjs.extend([right for right in tok.rights if tok.dep_ in ADJECTIVES])\n",
    "        tok_with_adj = \" \".join([adj.lower_ for adj in adjs])\n",
    "        toks_with_adjectives.extend(adjs)\n",
    "\n",
    "    return toks_with_adjectives\n",
    "\n",
    "def getObjsFromAttrs(deps):\n",
    "    for dep in deps:\n",
    "        if dep.pos_ == \"NOUN\" and dep.dep_ == \"attr\":\n",
    "            verbs = [tok for tok in dep.rights if tok.pos_ == \"VERB\"]\n",
    "            if len(verbs) > 0:\n",
    "                for v in verbs:\n",
    "                    rights = list(v.rights)\n",
    "                    objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
    "                    objs.extend(getObjsFromPrepositions(rights))\n",
    "                    if len(objs) > 0:\n",
    "                        return v, objs\n",
    "    return None, None\n",
    "\n",
    "def getObjFromXComp(deps):\n",
    "    for dep in deps:\n",
    "        if dep.pos_ == \"VERB\" and dep.dep_ == \"xcomp\":\n",
    "            v = dep\n",
    "            rights = list(v.rights)\n",
    "            objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
    "            objs.extend(getObjsFromPrepositions(rights))\n",
    "            if len(objs) > 0:\n",
    "                return v, objs\n",
    "    return None, None\n",
    "\n",
    "def getAllSubs(v):\n",
    "    verbNegated = isNegated(v)\n",
    "    subs = [tok for tok in v.lefts if tok.dep_ in SUBJECTS and tok.pos_ != \"DET\"]\n",
    "    if len(subs) > 0:\n",
    "        subs.extend(getSubsFromConjunctions(subs))\n",
    "    else:\n",
    "        foundSubs, verbNegated = findSubs(v)\n",
    "        subs.extend(foundSubs)\n",
    "    return subs, verbNegated\n",
    "\n",
    "def getAllObjs(v):\n",
    "    # rights is a generator\n",
    "    rights = list(v.rights)\n",
    "    objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
    "    objs.extend(getObjsFromPrepositions(rights))\n",
    "\n",
    "    potentialNewVerb, potentialNewObjs = getObjFromXComp(rights)\n",
    "    if potentialNewVerb is not None and potentialNewObjs is not None and len(potentialNewObjs) > 0:\n",
    "        objs.extend(potentialNewObjs)\n",
    "        v = potentialNewVerb\n",
    "    if len(objs) > 0:\n",
    "        objs.extend(getObjsFromConjunctions(objs))\n",
    "    return v, objs\n",
    "\n",
    "def getAllObjsWithAdjectives(v):\n",
    "    # rights is a generator\n",
    "    rights = list(v.rights)\n",
    "    objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
    "\n",
    "    if len(objs)== 0:\n",
    "        objs = [tok for tok in rights if tok.dep_ in ADJECTIVES]\n",
    "\n",
    "    objs.extend(getObjsFromPrepositions(rights))\n",
    "\n",
    "    potentialNewVerb, potentialNewObjs = getObjFromXComp(rights)\n",
    "    if potentialNewVerb is not None and potentialNewObjs is not None and len(potentialNewObjs) > 0:\n",
    "        objs.extend(potentialNewObjs)\n",
    "        v = potentialNewVerb\n",
    "    if len(objs) > 0:\n",
    "        objs.extend(getObjsFromConjunctions(objs))\n",
    "    return v, objs\n",
    "\n",
    "def findSVOs(tokens):\n",
    "    svos = []\n",
    "    verbs = [tok for tok in tokens if tok.pos_ == \"VERB\" and tok.dep_ != \"aux\"]\n",
    "    for v in verbs:\n",
    "        subs, verbNegated = getAllSubs(v)\n",
    "        # hopefully there are subs, if not, don't examine this verb any longer\n",
    "        if len(subs) > 0:\n",
    "            v, objs = getAllObjs(v)\n",
    "            for sub in subs:\n",
    "                for obj in objs:\n",
    "                    objNegated = isNegated(obj)\n",
    "                    svos.append((sub.lower_, \"!\" + v.lower_ if verbNegated or objNegated else v.lower_, obj.lower_))\n",
    "    return svos\n",
    "\n",
    "def findSVAOs(tokens):\n",
    "    svos = []\n",
    "    verbs = [tok for tok in tokens if tok.pos_ == \"VERB\" and tok.dep_ != \"aux\"]\n",
    "    for v in verbs:\n",
    "        subs, verbNegated = getAllSubs(v)\n",
    "        # hopefully there are subs, if not, don't examine this verb any longer\n",
    "        if len(subs) > 0:\n",
    "            v, objs = getAllObjsWithAdjectives(v)\n",
    "            for sub in subs:\n",
    "                for obj in objs:\n",
    "                    objNegated = isNegated(obj)\n",
    "                    obj_desc_tokens = generate_left_right_adjectives(obj)\n",
    "                    sub_compound = generate_sub_compound(sub)\n",
    "                    svos.append((\" \".join(tok.lower_ for tok in sub_compound), \"!\" + v.lower_ if verbNegated or objNegated else v.lower_, \" \".join(tok.lower_ for tok in obj_desc_tokens)))\n",
    "    return svos\n",
    "\n",
    "def generate_sub_compound(sub):\n",
    "    sub_compunds = []\n",
    "    for tok in sub.lefts:\n",
    "        if tok.dep_ in COMPOUNDS:\n",
    "            sub_compunds.extend(generate_sub_compound(tok))\n",
    "    sub_compunds.append(sub)\n",
    "    for tok in sub.rights:\n",
    "        if tok.dep_ in COMPOUNDS:\n",
    "            sub_compunds.extend(generate_sub_compound(tok))\n",
    "    return sub_compunds\n",
    "\n",
    "def generate_left_right_adjectives(obj):\n",
    "    obj_desc_tokens = []\n",
    "    for tok in obj.lefts:\n",
    "        if tok.dep_ in ADJECTIVES:\n",
    "            obj_desc_tokens.extend(generate_left_right_adjectives(tok))\n",
    "    obj_desc_tokens.append(obj)\n",
    "\n",
    "    for tok in obj.rights:\n",
    "        if tok.dep_ in ADJECTIVES:\n",
    "            obj_desc_tokens.extend(generate_left_right_adjectives(tok))\n",
    "\n",
    "    return obj_desc_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "# parser = English()\n",
    "parser = spacy.load('en', disable=['ner','textcat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parse = parser(legends[4])\n",
    "print(findSVOs(parse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for legend in parser(legends[0:4]):\n",
    "#     print(legend)\n",
    "#     parse = parser(legend)\n",
    "    print(findSVOs(parse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
